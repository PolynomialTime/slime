# HH-RLHF æ•°æ®é›†è®­ç»ƒæŒ‡å—

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨ slime æ¡†æ¶åœ¨ HH-RLHF æ•°æ®é›†ä¸Šè¿›è¡Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

## ğŸ“ æ–‡ä»¶è¯´æ˜

- `slime/` - slime æ¡†æ¶ä¸»ç›®å½•
- `hh-rlhf/` - åŸå§‹ HH-RLHF æ•°æ®é›†
- `hh-rlhf-processed/` - å¤„ç†åçš„è®­ç»ƒæ•°æ®
- `prepare_hh_rlhf.py` - æ•°æ®é¢„å¤„ç†è„šæœ¬
- `run-hh-rlhf-training.sh` - è®­ç»ƒå¯åŠ¨è„šæœ¬

## ğŸ“Š æ•°æ®é›†ç»Ÿè®¡

å·²æˆåŠŸä¸‹è½½å¹¶å¤„ç† HH-RLHF æ•°æ®é›†ï¼š

| æ•°æ®é›† | è®­ç»ƒé›† | æµ‹è¯•é›† |
|--------|--------|--------|
| helpful-base | 43,834 | 2,354 |
| harmless-base | 42,491 | 2,308 |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

#### æ–¹å¼ä¸€ï¼šä½¿ç”¨ Dockerï¼ˆæ¨èï¼‰

```bash
# æ‹‰å– slime å®˜æ–¹é•œåƒ
docker pull slimerl/slime:latest

# å¯åŠ¨å®¹å™¨ï¼ˆå°†å½“å‰ç›®å½•æŒ‚è½½åˆ°å®¹å™¨ï¼‰
docker run --rm --gpus all --ipc=host --shm-size=16g \
  --ulimit memlock=-1 --ulimit stack=67108864 \
  -v $(pwd):/workspace \
  -it slimerl/slime:latest /bin/bash

# åœ¨å®¹å™¨å†…æ›´æ–° slime
cd /workspace/slime
git pull
pip install -e . --no-deps
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨ Conda

```bash
# å‚è€ƒ slime çš„ build_conda.sh è„šæœ¬
cd slime
bash build_conda.sh
```

### 2. ä¸‹è½½å’Œå‡†å¤‡æ¨¡å‹

é€‰æ‹©ä¸€ä¸ªåŸºç¡€æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œä¾‹å¦‚ Qwen3-4B æˆ– GLM4-9Bï¼š

```bash
# ç¤ºä¾‹ï¼šä¸‹è½½ Qwen3-4B æ¨¡å‹
hf download Qwen/Qwen3-4B --local-dir /path/to/Qwen3-4B

# è½¬æ¢ä¸º Megatron æ ¼å¼
cd slime
source scripts/models/qwen3-4B.sh

PYTHONPATH=/root/Megatron-LM python tools/convert_hf_to_torch_dist.py \
  ${MODEL_ARGS[@]} \
  --hf-checkpoint /path/to/Qwen3-4B \
  --save /path/to/Qwen3-4B_torch_dist
```

### 3. é…ç½®è®­ç»ƒè„šæœ¬

ç¼–è¾‘ `run-hh-rlhf-training.sh`ï¼Œä¿®æ”¹ä»¥ä¸‹å…³é”®é…ç½®ï¼š

```bash
# 1. åŠ è½½æ¨¡å‹é…ç½®ï¼ˆå–æ¶ˆæ³¨é‡Šå¯¹åº”çš„æ¨¡å‹ï¼‰
source "${SCRIPT_DIR}/slime/scripts/models/qwen3-4B.sh"

# 2. è®¾ç½®æ¨¡å‹è·¯å¾„
CKPT_ARGS=(
   --hf-checkpoint /path/to/Qwen3-4B
   --ref-load /path/to/Qwen3-4B_torch_dist
   --load /path/to/Qwen3-4B_slime/
   --save /path/to/Qwen3-4B_slime/
   --save-interval 20
)

# 3. æ ¹æ®ä½ çš„ GPU æ•°é‡è°ƒæ•´å¹¶è¡Œé…ç½®
--actor-num-gpus-per-node 4  # è®­ç»ƒä½¿ç”¨çš„ GPU æ•°
--rollout-num-gpus 4          # æ¨ç†ä½¿ç”¨çš„ GPU æ•°
```

### 4. å¯åŠ¨è®­ç»ƒ

```bash
# ç»™è„šæœ¬æ·»åŠ æ‰§è¡Œæƒé™
chmod +x run-hh-rlhf-training.sh

# å¯åŠ¨è®­ç»ƒ
bash run-hh-rlhf-training.sh
```

## ğŸ”§ å…³é”®å‚æ•°è¯´æ˜

### æ•°æ®ç›¸å…³
- `--prompt-data`: è®­ç»ƒæ•°æ®è·¯å¾„
- `--input-key`: è¾“å…¥å­—æ®µåï¼ˆé»˜è®¤ä¸º `text`ï¼Œå¯çœç•¥ï¼‰
- `--label-key`: æ ‡ç­¾å­—æ®µåï¼ˆé»˜è®¤ä¸º `label`ï¼Œå¯é€‰ï¼‰
- `--apply-chat-template`: åº”ç”¨å¯¹è¯æ¨¡æ¿

**æ³¨æ„**ï¼šæœ¬é¡¹ç›®ä½¿ç”¨ slime æ¡†æ¶çš„æ ‡å‡†å­—æ®µå `text` å’Œ `label`ï¼Œå› æ­¤ä¸éœ€è¦æ˜¾å¼æŒ‡å®š `--input-key` å‚æ•°ã€‚

### è®­ç»ƒæ§åˆ¶
- `--num-rollout`: æ€»è®­ç»ƒè½®æ¬¡
- `--rollout-batch-size`: æ¯è½®é‡‡æ ·çš„ prompt æ•°é‡
- `--n-samples-per-prompt`: æ¯ä¸ª prompt ç”Ÿæˆçš„å›å¤æ•°é‡
- `--global-batch-size`: å‚æ•°æ›´æ–°çš„æ‰¹æ¬¡å¤§å°

**é‡è¦çº¦æŸ**ï¼š
```
rollout-batch-size Ã— n-samples-per-prompt = global-batch-size Ã— num-steps-per-rollout
```

### å¹¶è¡Œé…ç½®
- `--tensor-model-parallel-size`: å¼ é‡å¹¶è¡Œåº¦
- `--pipeline-model-parallel-size`: æµæ°´çº¿å¹¶è¡Œåº¦
- `--context-parallel-size`: ä¸Šä¸‹æ–‡å¹¶è¡Œåº¦

### GRPO ç®—æ³•
- `--advantage-estimator`: ä¼˜åŠ¿ä¼°è®¡å™¨ï¼ˆgrpo/gspo/ppoï¼‰
- `--kl-loss-coef`: KL æ•£åº¦æŸå¤±ç³»æ•°
- `--eps-clip`: PPO è£å‰ªå‚æ•°

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### ä½¿ç”¨ Ray Dashboard
è®­ç»ƒå¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡æµè§ˆå™¨è®¿é—®ï¼š
```
http://localhost:8265
```

### ä½¿ç”¨ Weights & Biasesï¼ˆå¯é€‰ï¼‰
åœ¨ `run-hh-rlhf-training.sh` ä¸­å¯ç”¨ wandbï¼š
```bash
WANDB_ARGS=(
   --use-wandb
   --wandb-project hh-rlhf-training
   --wandb-group my-experiment
   --wandb-key ${WANDB_KEY}
)
```

## ğŸ¯ è®­ç»ƒåæ“ä½œ

### è½¬æ¢æ¨¡å‹å› HuggingFace æ ¼å¼

```bash
cd slime

PYTHONPATH=/root/Megatron-LM python tools/convert_torch_dist_to_hf.py \
  --input-dir /path/to/model_slime/iter_xxx/ \
  --output-dir /path/to/model_hf_iter_xxx \
  --origin-hf-dir /path/to/original_model
```

## ğŸ’¡ å¸¸è§é—®é¢˜

### 1. æ˜¾å­˜ä¸è¶³
- å‡å° `--max-tokens-per-gpu`
- å¢åŠ  `--tensor-model-parallel-size`
- å¯ç”¨ `--recompute-granularity full`

### 2. è®­ç»ƒé€Ÿåº¦æ…¢
- å¯ç”¨ `--use-dynamic-batch-size`
- è°ƒæ•´ `--rollout-batch-size` å’Œ `--global-batch-size`
- æ£€æŸ¥ `--balance-data` æ˜¯å¦å¯ç”¨

### 3. è®­æ¨ä¸€ä½“åŒ–æ¨¡å¼
å¦‚æœ GPU æ•°é‡æœ‰é™ï¼Œå¯ä»¥ä½¿ç”¨ colocated æ¨¡å¼ï¼š
```bash
ray job submit ... \
  -- python3 train.py \
  --actor-num-gpus-per-node 8 \
  --colocate \
  --sglang-mem-fraction-static 0.8 \
  ...
```

## ğŸ“š å‚è€ƒèµ„æº

- [slime å®˜æ–¹æ–‡æ¡£](https://github.com/THUDM/slime)
- [HH-RLHF æ•°æ®é›†](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [GRPO è®ºæ–‡](https://arxiv.org/abs/2402.03300)
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [SGLang](https://github.com/sgl-project/sglang)

## ğŸ”„ æ•°æ®é›†å˜ä½“

å¦‚æœæƒ³ä½¿ç”¨å…¶ä»– HH-RLHF å­é›†ï¼Œä¿®æ”¹è®­ç»ƒè„šæœ¬ä¸­çš„æ•°æ®è·¯å¾„ï¼š

```bash
# ä½¿ç”¨ harmless-base
--prompt-data ${SCRIPT_DIR}/hh-rlhf-processed/harmless-base-train.jsonl

# ä½¿ç”¨ helpful-online
--prompt-data ${SCRIPT_DIR}/hh-rlhf-processed/helpful-online-train.jsonl
```

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **ç¡¬ä»¶è¦æ±‚**ï¼šå»ºè®®ä½¿ç”¨ H100/H200 æˆ– B200 ç³»åˆ— GPU
2. **æ¨¡å‹é…ç½®**ï¼šç¡®ä¿æ¨¡å‹é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°ä¸å®é™…æ¨¡å‹åŒ¹é…
3. **è·¯å¾„è®¾ç½®**ï¼šæ‰€æœ‰è·¯å¾„éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´
4. **Docker ç¯å¢ƒ**ï¼šæ¨èä½¿ç”¨å®˜æ–¹ Docker é•œåƒä»¥é¿å…ä¾èµ–é—®é¢˜

---

**ç¥è®­ç»ƒé¡ºåˆ©ï¼** ğŸ‰
